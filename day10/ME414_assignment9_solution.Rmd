---
title: "Exercise 9 - Topic Models"
author: "Jack Blumenau, Ken Benoit & Slava Mikhaylov"
output: html_document
---

You will need to load the following libraries (you may also want to set the random number seed to make everything replicable):
```{r, eval=T, message = F}
library(quanteda)
library(topicmodels)
library(LDAvis)
library(stm)
library(knitr)
library(lda)
library(servr)
set.seed(221186)
```

## Exercise 9.1

In this question we are going to use topic modelling to understand how parliamentary speech varies by the gender of the MP. We will be working with a corpus of speeches made by legislators in the UK House of Commons in the 2014 calandar year. You will need to make sure that the file `hoc_speeches.Rdata` is in your current working directory, and then use the following command to read this data into `R`.


```{r, message = FALSE}

load("hoc_speeches.Rdata")

```
 
 (a) Inspect the `data.frame` object `speeches` and produce some summary statistics.

```{r}

prop.table(table(speeches$party, speeches$gender),1)

speeches$ntoken <- ntoken(speeches$speech)
hist(speeches$ntoken, main = "Distribution of speech length", breaks = 100)

```

 (a) Use the functions in the `quanteda` package to turn this data into a `corpus` object. Attach the relevant metadata as `docvars`.

```{r}

speechCorpus <- corpus(speeches$speech, docvars = speeches)

```

 (b) Turn this corpus into a document-feature matrix. At a minimum, you should remove punctuation and numbers from the texts when constructing the `dfm` (`remove_punct =T` & `remove_numbers = T`) but you may also want to do some additional pre-processing if you don't want to wait days for your topic model to coverge. Think about some of the following:
 
    (i) Unigrams? 
    (ii) Stopwords?
    (iii) Stemming?
    (iv) Very infrequent words?

```{r, message = FALSE}

speechDFM <- dfm(speechCorpus, remove = stopwords("en"), remove_punct =T, remove_numbers = T, stem = T)

speechDFM <- dfm_trim(speechDFM, min_termfreq = 5, min_docfreq = 0.0025, docfreq_type = "prop")

```

 (c) Run a structural topic model for this corpus, using the `gender` variable in the topic prevalence argument. Use the `stm` function to do this. Set the `seed` argument to `stm` to be equal to `123`. Be aware, this takes about 15 minutes to run on Jack's laptop -- for testing purposes you might want to set the maximum iterations for the stm to be some low number (`max.em.its = 10` for instance).

Now specify and estimate the `stm` model:

```{r, cache = T, message=FALSE}

K <- 20
stmOut <- stm(documents = speechDFM, 
              data = docvars(speechDFM),
              prevalence = ~gender,
              K = K, seed = 123, verbose = FALSE, max.em.its = 5)

```

Plot the estimated topic model:

```{r}
plot(stmOut)
```

  (d) Examine the top words from each topic

```{r}

topic_labels <- labelTopics(stmOut)
topic_labels <- apply(topic_labels$prob,1, function(x) paste(x, collapse=";"))
print(topic_labels)
```


  (e) Find the top three documents associated with each topic. Do these make sense given the words you have used to describe that topic? (Hint: in the estimated `stm` object, the document-topic probabilities are stored in `theta`) Report the top speeches for one selected topic.
  
```{r}

top_docs <- apply(stmOut$theta, 2, function(x) order(x, decreasing = T)[1:3])

top_school_docs <- top_docs[,grep("school",topic_labels)]

docvars(speechDFM)[top_school_docs,"speech"]

```

  (f) Use the `estimateEffect` and `plot.estimateEffect` functions in the `stm` package to estimate the effect of MP gender on topic usage. On which topics are women, on average, more active? 

```{r}

est_gender_effect <- estimateEffect(~gender, stmOut, metadata = docvars(speechDFM))

plot.estimateEffect(est_gender_effect, "gender", method = "difference", 
                    cov.value1 = "female", cov.value2 = "male", 
                    labeltype = "frex", n = 3, verbose.labels = F,
                    model = stmOut)

```

**Women appear to speak more about the `women,world,eu`, and `health,nhs,hospit` topics, though the significance of these is effects in this data is questionable.**

2.  **movies corpus**.  Here we will use the very impressive `LDAvis` library in conjunction with the `lda::lda.collapsed.gibbs.sampler()` function from the `lda` package. The following code is used to demonstate how the parliamentary speeches interactive visualisation example was created for in the lecture. Your task is to implement this for the `movies` corpus.

First we construct the relevant `dfm` and estimate the `lda` model.
```{r, eval=FALSE}

## Create a corpus of speeches
speechCorpus <- corpus(speeches$speech)

## Convert to dfm, removing some words that appear very regularly
speechDfm <- dfm(speechCorpus, remove = c(stopwords("en"), "will", "hon", "right","people","government","can","friend","house","gentleman","said", "interruption", "prime", "minister", "secretary", "state"), stem = F, remove_punct = T, remove_numbers = T)

## Trim some rarely occuring words
speechDfm <- dfm_trim(speechDfm, min_termfreq = 15, min_docfreq = 0.0015, docfreq_type = "prop")

# Convert to lda format
speechDfmlda <- convert(speechDfm, to = "lda")

# MCMC and model tuning parameters:
K <- 30 # Number of topics
G <- 20 # Number of iterations
alpha <- 0.02 # Prior for topic proportions
eta <- 0.02 # Prior for topic distributions

# Fit the model
t1 <- Sys.time() # Start timer

fit <- lda.collapsed.gibbs.sampler(documents = speechDfmlda$documents, K = K, 
                                   vocab = speechDfmlda$vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time() # End timer
t2 - t1  # about 15 minutes on Jack's MacBook Pro


```

Now we plot the model using `LDAvis`.

```{r, eval=FALSE}
library(LDAvis)
# create the JSON object to feed the visualization:
json <- createJSON(phi = t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x))), 
                   theta = t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x))), 
                   doc.length = ntoken(speechDfm), 
                   vocab = featnames(speechDfm), 
                   term.frequency = colSums(speechDfm))
        
serVis(json, out.dir = "exampleVis", open.browser = TRUE)
```

  a.  You will need to load the data from the `quanteda.corpora` package which is hosted on GitHub: 
    
```{r}
  library(devtools)
  install_github("quanteda/quanteda.corpora")
  data(data_corpus_movies, package = "quanteda.corpora")
  
```
    
  b.  Adapt the code above to produce an interactive visualisation of the `movies` corpus. 
    
```{r, cache = TRUE, eval = FALSE}

      
# prepare the texts
moviesDfm <- dfm(data_corpus_movies, remove = stopwords("en"), stem = FALSE, remove_punct = T, remove_numbers = T)
moviesDfm <- dfm_trim(moviesDfm, min_termfreq = 5)

# MCMC and model tuning parameters:
K <- 20
G <- 20
alpha <- 0.02
eta <- 0.02

# convert to lda format
moviesDfmlda <- convert(moviesDfm, to = "lda")
# fit the model
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = moviesDfmlda$documents, K = K, 
                                   vocab = moviesDfmlda$vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 10 minutes on Jack's iMac

library(LDAvis)
# create the JSON object to feed the visualization:
json <- createJSON(phi = t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x))), 
                   theta = t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x))), 
                   doc.length = ntoken(moviesDfm), 
                   vocab = featnames(moviesDfm), 
                   term.frequency = colSums(moviesDfm))

serVis(json, out.dir = "visColl", open.browser = TRUE)

```
    
  d.  Describe a few topics as you see them.  Is there a "scary movie" topic?  Is there a "science fiction" topic?  
    
    